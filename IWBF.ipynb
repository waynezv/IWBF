{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech latent feature discovery for multi-label forensic profiling\n",
    "\n",
    "Project for IWBF 2018\n",
    "\n",
    "----\n",
    "\n",
    "**Objective:** Find latent representations in speech that can predict forensic aspects of human.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update ~12/11/2017\n",
    "\n",
    "1. Write-up:\n",
    "\n",
    "    https://www.overleaf.com/12702004wrrmdybmpqsq#/48448185/\n",
    "\n",
    "2. Implementation\n",
    "    1. Data: TIMIT, Alcohol, (SRE)\n",
    "    2. Model\n",
    "        ![model](./write-up/figs/model.png)  \n",
    "    3. Current attributes: speaker-id, gender, dialect, age, height, drunk\n",
    "    4. [Code](./src)\n",
    "    \n",
    "3. Final poster for convex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update ~12/18/2017\n",
    "\n",
    "1. Experiments\n",
    "    1. Currently using latent codes from autoencoder to do multi-label classification and regression.\n",
    "    2. It works well on certain tasks (reconstruction, gender, height), but not well on other tasks (id, dialect, age).\n",
    "    3. Including more datasets (SRE, Yandong's face data)\n",
    "\n",
    "large model openset (100 epochs, batch 4)\n",
    "\n",
    "|Label | Reconstruction Loss ($l_2^2$) | ID Acc (%) | Gender Acc (%) | Dialect Acc (%) | Age MSE | Height MSE |\n",
    "|:----|----:|---:|---:|---:|----:|---:|\n",
    "| Train | 0.1417 | 75.15 | 97.15 | 80.40 | 6.6931 | 0.0668 |\n",
    "| Test | 0.1847 | 3.59 | 92.83 | 14.73 | 81.1115 | 0.0818 |\n",
    " \n",
    "2. Writing paper\n",
    "\n",
    "3. Todo next week\n",
    "    1. Finish draft of the paper.\n",
    "    2. Improve the model.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update ~12/19/2017\n",
    "\n",
    "1. Experiments\n",
    "    1. Observations: ID, Age and Dialect seem to be correlated (similar trends of loss), whereas Gender and Height seem to be independent (not speaker-dependent).\n",
    "    2. Back to closed set of speakers -- openset is far more difficult.\n",
    "    3. Reduce model size to speed up training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update ~12/20/2017\n",
    "\n",
    "1. Experiments\n",
    "    1. Train on size-reduced model: significantly reduced model size, less layers, less parameters, smaller z dimension.\n",
    "    2. Use closed set speakers, split (0.8 / 0.2).\n",
    "    3. Observations: \n",
    "    \n",
    "small model openset (300 epochs, batch 4)\n",
    "\n",
    "|Label | Reconstruction Loss ($l_2^2$) | ID Err (%) | Gender Err (%) | Dialect Err (%) | Age MSE | Height MSE |\n",
    "|:----|----:|---:|---:|---:|----:|---:|\n",
    "| Train | 0.1168 | 0.98 | 0.02 | 1.45 | 1.1543 | 0.0980 |\n",
    "| Test | 0.1529 | 98.60 | 3.81 |  82.36 | 83.5361 | 0.0981 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update ~12/20/2017\n",
    "\n",
    "1. Experiments\n",
    "    1. Observations: while the train losses monotonically decreases, the test losses for id and dialect strangely increases.\n",
    "\n",
    "![strange-loss](./write-up/figs/strange_loss.png)\n",
    "\n",
    "small model closedset (300 epochs, batch 16)\n",
    "\n",
    "|Label | Reconstruction Loss ($l_2^2$) | ID Err (%) | Gender Err (%) | Dialect Err (%) | Age MSE | Height MSE |\n",
    "|:----|----:|---:|---:|---:|----:|---:|\n",
    "| Train | 0.1209 | 0.15 | 0.00 | 0.31 | 0.5237 | 0.0177 |\n",
    "| Test | 0.1609 | 87.54 | 0.0150 | 77.97 | 60.2400 | 0.0773 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update ~12/28/2017\n",
    "\n",
    "Did not do much this week ...\n",
    "\n",
    "1. Found the stupid bug in code, now all losses decreasing.\n",
    "    \n",
    "small model closedset\n",
    "\n",
    "|Label | Reconstruction Loss ($l_2^2$) | ID Err (%) | Gender Err (%) | Dialect Err (%) | Age MSE | Height MSE |\n",
    "|:----|----:|---:|---:|---:|----:|---:|\n",
    "| Train |  | 32.40 |  |  |  |  |\n",
    "| Test |  | 48.33 | | |  ||\n",
    "\n",
    "2. Paper\n",
    "\n",
    "    https://www.overleaf.com/12702004wrrmdybmpqsq#/48448185/\n",
    "\n",
    "3. Todo\n",
    "\n",
    "    1. Extend to multi-datasets\n",
    "    2. Have to finish the paper !!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update ~12/30/2017\n",
    "\n",
    "1. Multitask learning\n",
    "    1. Models\n",
    "        1. Neural models\n",
    "            1. Hard parameter sharing\n",
    "            2. Soft parameter sharing\n",
    "\n",
    "            ** sharing information with unrelated tasks might impede learning individual tasks**\n",
    "        2. Non-neural models [assume model parameter $W$ of size ($d$-feature dims $\\times$ $T$-tasks)]\n",
    "            1. sparsity across tasks through norm regularization\n",
    "                1. block-sparse regularization: mixed $l_1 / l_q$ norms\n",
    "                2. group lasso\n",
    "                3. trace norm regularize: low rank\n",
    "                4. combine block-sparse and element-wise sparse\n",
    "            2. modelling the relationships between tasks\n",
    "                1. clustering constraints: task mean and between-task variance; possibly add within-task variance\n",
    "                2. more complex structures: graph, tree, k-nn\n",
    "                3. Bayesian\n",
    "                    1. Gaussian process with shared covariance\n",
    "                    2. mean task-dependent and a clustering of the tasks using a mixture distribution\n",
    "                    3. Dirichlet process and enable the model to learn the similarity between tasks as well as the number of clusters\n",
    "                    4. hierarchical Bayesian model -- a latent task hierarchy\n",
    "                    5. actual tasks are linear combination of small number of latent basis tasks\n",
    "        3. Auxiliary tasks\n",
    "            1. related task\n",
    "            2. adversarial task\n",
    "            3. quantization smoothing\n",
    "            4. representation learning\n",
    "\n",
    "2. Multitask learning and Adversarial learning\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech latent feature discovery for multi-label forensic profiling\n",
    "\n",
    "Project for IWBF 2018\n",
    "\n",
    "----\n",
    "\n",
    "**Objective:** Find latent representations in speech that can predict forensic aspects of human.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update ~12/11/2017\n",
    "\n",
    "1. Write-up:\n",
    "\n",
    "    https://www.overleaf.com/12702004wrrmdybmpqsq#/48448185/\n",
    "\n",
    "2. Implementation\n",
    "    1. Data: TIMIT, Alcohol, (SRE)\n",
    "    2. Model\n",
    "        ![model](./write-up/figs/model.png)  \n",
    "    3. Current attributes: speaker-id, gender, dialect, age, height, drunk\n",
    "    4. [Code](./src)\n",
    "    \n",
    "3. Final poster for convex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update ~12/18/2017\n",
    "\n",
    "1. Experiments\n",
    "    1. Currently using latent codes from autoencoder to do multi-label classification and regression.\n",
    "    2. It works well on certain tasks (reconstruction, gender, height), but not well on other tasks (id, dialect, age).\n",
    "    3. Including more datasets (SRE, Yandong's face data)\n",
    "\n",
    "large model openset (100 epochs, batch 4)\n",
    "\n",
    "|Label | Reconstruction Loss ($l_2^2$) | ID Acc (%) | Gender Acc (%) | Dialect Acc (%) | Age MSE | Height MSE |\n",
    "|:----|----:|---:|---:|---:|----:|---:|\n",
    "| Train | 0.1417 | 75.15 | 97.15 | 80.40 | 6.6931 | 0.0668 |\n",
    "| Test | 0.1847 | 3.59 | 92.83 | 14.73 | 81.1115 | 0.0818 |\n",
    " \n",
    "2. Writing paper\n",
    "\n",
    "3. Todo next week\n",
    "    1. Finish draft of the paper.\n",
    "    2. Improve the model.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update ~12/19/2017\n",
    "\n",
    "1. Experiments\n",
    "    1. Observations: ID, Age and Dialect seem to be correlated (similar trends of loss), whereas Gender and Height seem to be independent (not speaker-dependent).\n",
    "    2. Back to closed set of speakers -- openset is far more difficult.\n",
    "    3. Reduce model size to speed up training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update ~12/20/2017\n",
    "\n",
    "1. Experiments\n",
    "    1. Train on size-reduced model: significantly reduced model size, less layers, less parameters, smaller z dimension.\n",
    "    2. Use closed set speakers, split (0.8 / 0.2).\n",
    "    3. Observations: \n",
    "    \n",
    "small model openset (300 epochs, batch 4)\n",
    "\n",
    "|Label | Reconstruction Loss ($l_2^2$) | ID Err (%) | Gender Err (%) | Dialect Err (%) | Age MSE | Height MSE |\n",
    "|:----|----:|---:|---:|---:|----:|---:|\n",
    "| Train | 0.1168 | 0.98 | 0.02 | 1.45 | 1.1543 | 0.0980 |\n",
    "| Test | 0.1529 | 98.60 | 3.81 |  82.36 | 83.5361 | 0.0981 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update ~12/20/2017\n",
    "\n",
    "1. Experiments\n",
    "    1. Observations: while the train losses monotonically decreases, the test losses for id and dialect strangely increases.\n",
    "\n",
    "![strange-loss](./write-up/figs/strange_loss.png)\n",
    "\n",
    "small model closedset (300 epochs, batch 16)\n",
    "\n",
    "|Label | Reconstruction Loss ($l_2^2$) | ID Err (%) | Gender Err (%) | Dialect Err (%) | Age MSE | Height MSE |\n",
    "|:----|----:|---:|---:|---:|----:|---:|\n",
    "| Train | 0.1209 | 0.15 | 0.00 | 0.31 | 0.5237 | 0.0177 |\n",
    "| Test | 0.1609 | 87.54 | 0.0150 | 77.97 | 60.2400 | 0.0773 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update ~12/28/2017\n",
    "\n",
    "Did not do much this week ...\n",
    "\n",
    "1. Found the stupid bug in code, now all losses decreasing.\n",
    "    \n",
    "small model closedset\n",
    "\n",
    "|Label | Reconstruction Loss ($l_2^2$) | ID Err (%) | Gender Err (%) | Dialect Err (%) | Age MSE | Height MSE |\n",
    "|:----|----:|---:|---:|---:|----:|---:|\n",
    "| Train |  | 32.40 |  |  |  |  |\n",
    "| Test |  | 48.33 | | |  ||\n",
    "\n",
    "2. Paper\n",
    "\n",
    "    https://www.overleaf.com/12702004wrrmdybmpqsq#/48448185/\n",
    "\n",
    "3. Todo\n",
    "\n",
    "    1. Extend to multi-datasets\n",
    "    2. Have to finish the paper !!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Update ~12/30/2017\n",
    "\n",
    "1. Multitask learning (MTL)\n",
    "    1. Models\n",
    "        1. Neural models\n",
    "            1. Hard parameter sharing\n",
    "            2. Soft parameter sharing\n",
    "            3. **what to share in model?**\n",
    "\n",
    "                ** sharing information with unrelated tasks might impede learning individual tasks**\n",
    "        2. Non-neural models [assume model parameter $W$ of size ($d$-feature dims $\\times$ $T$-tasks)]\n",
    "            1. sparsity across tasks through norm regularization\n",
    "                1. block-sparse regularization: mixed $l_1 / l_q$ norms\n",
    "                2. group lasso\n",
    "                3. trace norm regularize: low rank\n",
    "                4. combine block-sparse and element-wise sparse\n",
    "            2. modelling the relationships between tasks\n",
    "                1. clustering constraints: task mean and between-task variance; possibly add within-task variance\n",
    "                2. more complex structures: graph, tree, k-nn\n",
    "                3. Bayesian\n",
    "                    1. Gaussian process with shared covariance\n",
    "                    2. mean task-dependent and a clustering of the tasks using a mixture distribution\n",
    "                    3. Dirichlet process and enable the model to learn the similarity between tasks as well as the number of clusters\n",
    "                    4. hierarchical Bayesian model -- a latent task hierarchy\n",
    "                    5. actual tasks are linear combination of small number of latent basis tasks\n",
    "        3. Auxiliary tasks\n",
    "            1. related task\n",
    "            2. adversarial task\n",
    "            3. hints: predict features\n",
    "            4. attention\n",
    "            5. quantization smoothing\n",
    "            6. representation learning\n",
    "            7. **what auxiliary tasks are helpful?**\n",
    "            \n",
    "            e.g.  two tasks are $\\mathcal{F}$-related if the data for both tasks can be generated from a fixed probability distribution using a set of transformations $\\mathcal{F}$\n",
    "            \n",
    "            Task similarity is not binary, but resides on a spectrum. More similar tasks should help more in MTL, while less similar tasks should help less. Allowing our models to learn what to share with each task might allow us to temporarily circumvent the lack of theory and make better use even of only loosely related tasks. However, we also need to develop a more principled notion of task similarity with regard to multi-task learning in order to know which tasks we should prefer.\n",
    "            \n",
    "             **Existing problems**: task similarity, relationship, hierarchy, and benefit for MTL\n",
    "\n",
    "2. Multitask learning and Adversarial learning\n",
    "    1. Domain adaptation ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update ~01/01/2018\n",
    "\n",
    "1. Address the problems analyzed in **Update ~12/30/2017**\n",
    "    1. Need a sharing mechanism among tasks\n",
    "    2. Need a latent domain adaptation mechanism\n",
    "    3. Weighted loss w.r.t. the task uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update ~01/02/2018\n",
    "\n",
    "1. Experiments\n",
    "    0. **Two difficulties**\n",
    "        1. Model was not able to handle single but difficult task (e.g., id, age), neither multiple tasks. --> need better model\n",
    "        2. The tasks are loosely relevant. Some tasks bias other tasks. --> need proper model sharing, loss weighting, auxillary tasks\n",
    "    1. Tried different sharing mechanisms but not work\n",
    "        1. sharing part of encoder but with different latent codes for different tasks\n",
    "        2. sharing latent codes (including varying latent code distributions, e.g. Gaussians with centered mean but different variances)\n",
    "        3. sharing part of decoder\n",
    "    2. Changed model to Resnet-like structure\n",
    "        1. combine multi-resolution features\n",
    "    3. Applying weighting to different task losses\n",
    "        1. hard-assigned weights: assign larger weights to tough tasks.\n",
    "        2. soft-assigned weights: probabilistically reformulate the task objectives (for regression, model likelihood as a Gaussian; for classification, use softmax), and then use objective variance / uncertainty to adjust weights.\n",
    "    4. Trying multi-stage autoencoders\n",
    "    5. Attention model?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech latent feature discovery for multi-label forensic profiling\n",
    "\n",
    "Project for IWBF 2018\n",
    "\n",
    "----\n",
    "\n",
    "**Objective:** Find latent representations in speech that can predict forensic aspects of human.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update ~12/11/2017\n",
    "\n",
    "1. Write-up:\n",
    "\n",
    "    https://www.overleaf.com/12702004wrrmdybmpqsq#/48448185/\n",
    "\n",
    "2. Implementation\n",
    "    1. Data: TIMIT, Alcohol, (SRE)\n",
    "    2. Model\n",
    "        ![model](./write-up/figs/model.png)  \n",
    "    3. Current attributes: speaker-id, gender, dialect, age, height, drunk\n",
    "    4. [Code](./src)\n",
    "    \n",
    "3. Final poster for convex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update ~12/18/2017\n",
    "\n",
    "1. Experiments\n",
    "    1. Currently using latent codes from autoencoder to do multi-label classification and regression.\n",
    "    2. It works well on certain tasks (reconstruction, gender, height), but not well on other tasks (id, dialect, age).\n",
    "    3. Including more datasets (SRE, Yandong's face data)\n",
    "\n",
    "large model openset (100 epochs, batch 4)\n",
    "\n",
    "|Label | Reconstruction Loss ($l_2^2$) | ID Acc (%) | Gender Acc (%) | Dialect Acc (%) | Age MSE | Height MSE |\n",
    "|:----|----:|---:|---:|---:|----:|---:|\n",
    "| Train | 0.1417 | 75.15 | 97.15 | 80.40 | 6.6931 | 0.0668 |\n",
    "| Test | 0.1847 | 3.59 | 92.83 | 14.73 | 81.1115 | 0.0818 |\n",
    " \n",
    "2. Writing paper\n",
    "\n",
    "3. Todo next week\n",
    "    1. Finish draft of the paper.\n",
    "    2. Improve the model.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update ~12/19/2017\n",
    "\n",
    "1. Experiments\n",
    "    1. Observations: ID, Age and Dialect seem to be correlated (similar trends of loss), whereas Gender and Height seem to be independent (not speaker-dependent).\n",
    "    2. Back to closed set of speakers -- openset is far more difficult.\n",
    "    3. Reduce model size to speed up training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update ~12/20/2017\n",
    "\n",
    "1. Experiments\n",
    "    1. Train on size-reduced model: significantly reduced model size, less layers, less parameters, smaller z dimension.\n",
    "    2. Use closed set speakers, split (0.8 / 0.2).\n",
    "    3. Observations: \n",
    "    \n",
    "small model openset (300 epochs, batch 4)\n",
    "\n",
    "|Label | Reconstruction Loss ($l_2^2$) | ID Err (%) | Gender Err (%) | Dialect Err (%) | Age MSE | Height MSE |\n",
    "|:----|----:|---:|---:|---:|----:|---:|\n",
    "| Train | 0.1168 | 0.98 | 0.02 | 1.45 | 1.1543 | 0.0980 |\n",
    "| Test | 0.1529 | 98.60 | 3.81 |  82.36 | 83.5361 | 0.0981 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update ~12/20/2017\n",
    "\n",
    "1. Experiments\n",
    "    1. Observations: while the train losses monotonically decreases, the test losses for id and dialect strangely increases.\n",
    "\n",
    "![strange-loss](./write-up/figs/strange_loss.png)\n",
    "\n",
    "small model closedset (300 epochs, batch 16)\n",
    "\n",
    "|Label | Reconstruction Loss ($l_2^2$) | ID Err (%) | Gender Err (%) | Dialect Err (%) | Age MSE | Height MSE |\n",
    "|:----|----:|---:|---:|---:|----:|---:|\n",
    "| Train | 0.1209 | 0.15 | 0.00 | 0.31 | 0.5237 | 0.0177 |\n",
    "| Test | 0.1609 | 87.54 | 0.0150 | 77.97 | 60.2400 | 0.0773 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update ~12/28/2017\n",
    "\n",
    "Did not do much this week ...\n",
    "\n",
    "1. Found the stupid bug in code, now all losses decreasing.\n",
    "    \n",
    "small model closedset\n",
    "\n",
    "|Label | Reconstruction Loss ($l_2^2$) | ID Err (%) | Gender Err (%) | Dialect Err (%) | Age MSE | Height MSE |\n",
    "|:----|----:|---:|---:|---:|----:|---:|\n",
    "| Train |  | 32.40 |  |  |  |  |\n",
    "| Test |  | 48.33 | | |  ||\n",
    "\n",
    "2. Paper\n",
    "\n",
    "    https://www.overleaf.com/12702004wrrmdybmpqsq#/48448185/\n",
    "\n",
    "3. Todo\n",
    "\n",
    "    1. Extend to multi-datasets\n",
    "    2. Have to finish the paper !!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Update ~12/30/2017\n",
    "\n",
    "1. Multitask learning (MTL)\n",
    "    1. Models\n",
    "        1. Neural models\n",
    "            1. Hard parameter sharing\n",
    "            2. Soft parameter sharing\n",
    "            3. **what to share in model?**\n",
    "\n",
    "                ** sharing information with unrelated tasks might impede learning individual tasks**\n",
    "        2. Non-neural models [assume model parameter $W$ of size ($d$-feature dims $\\times$ $T$-tasks)]\n",
    "            1. sparsity across tasks through norm regularization\n",
    "                1. block-sparse regularization: mixed $l_1 / l_q$ norms\n",
    "                2. group lasso\n",
    "                3. trace norm regularize: low rank\n",
    "                4. combine block-sparse and element-wise sparse\n",
    "            2. modelling the relationships between tasks\n",
    "                1. clustering constraints: task mean and between-task variance; possibly add within-task variance\n",
    "                2. more complex structures: graph, tree, k-nn\n",
    "                3. Bayesian\n",
    "                    1. Gaussian process with shared covariance\n",
    "                    2. mean task-dependent and a clustering of the tasks using a mixture distribution\n",
    "                    3. Dirichlet process and enable the model to learn the similarity between tasks as well as the number of clusters\n",
    "                    4. hierarchical Bayesian model -- a latent task hierarchy\n",
    "                    5. actual tasks are linear combination of small number of latent basis tasks\n",
    "        3. Auxiliary tasks\n",
    "            1. related task\n",
    "            2. adversarial task\n",
    "            3. hints: predict features\n",
    "            4. attention\n",
    "            5. quantization smoothing\n",
    "            6. representation learning\n",
    "            7. **what auxiliary tasks are helpful?**\n",
    "            \n",
    "            e.g.  two tasks are $\\mathcal{F}$-related if the data for both tasks can be generated from a fixed probability distribution using a set of transformations $\\mathcal{F}$\n",
    "            \n",
    "            Task similarity is not binary, but resides on a spectrum. More similar tasks should help more in MTL, while less similar tasks should help less. Allowing our models to learn what to share with each task might allow us to temporarily circumvent the lack of theory and make better use even of only loosely related tasks. However, we also need to develop a more principled notion of task similarity with regard to multi-task learning in order to know which tasks we should prefer.\n",
    "            \n",
    "             **Existing problems**: task similarity, relationship, hierarchy, and benefit for MTL\n",
    "\n",
    "2. **Multitask learning and Adversarial learning**\n",
    "    1. Domain adaptation ?\n",
    "    2. MTL with loosely related tasks: multiple adversarial learning?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update ~01/01/2018\n",
    "\n",
    "1. Address the problems analyzed in **Update ~12/30/2017**\n",
    "    1. Need a sharing mechanism among tasks\n",
    "    2. Need a latent domain adaptation mechanism\n",
    "    3. Weighted loss w.r.t. the task uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update ~01/02/2018\n",
    "\n",
    "1. Experiments\n",
    "    0. **Two difficulties**\n",
    "        1. Model was not able to handle single but difficult task (e.g., id, age), neither multiple tasks. --> need better model\n",
    "        2. The tasks are loosely relevant. Some tasks bias other tasks. --> need proper model sharing, loss weighting, auxillary tasks\n",
    "    1. Tried different sharing mechanisms but not work\n",
    "        1. sharing part of encoder but with different latent codes for different tasks\n",
    "        2. sharing latent codes (including varying latent code distributions, e.g. Gaussians with centered mean but different variances)\n",
    "        3. sharing part of decoder\n",
    "    2. Changed model to Resnet-like structure\n",
    "        1. combine multi-resolution features\n",
    "    3. Applying weighting to different task losses\n",
    "        1. hard-assigned weights: assign larger weights to tough tasks.\n",
    "        2. soft-assigned weights: probabilistically reformulate the task objectives (for regression, model likelihood as a Gaussian; for classification, use softmax), and then use objective variance / uncertainty to adjust weights.\n",
    "    4. Trying multi-stage autoencoders\n",
    "    5. Attention model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update ~01/03/2018\n",
    "\n",
    "1. Experiments\n",
    "    1. Resnet-like structure seems to improve accuracy than standard CNN.\n",
    "    2. Use task-wise learning rates and task-wise early stopping.\n",
    "    3. Use regularization on shared latent representation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update ~01/04/2018\n",
    "\n",
    "0. Conclusions\n",
    "    1. Based on previous observations and discussion, we need\n",
    "        0. Model: Resnet, probably add attention\n",
    "        1. Model sharing: must share from latent level\n",
    "        2. Related tasks: design auxillary tasks, corresponding loss and learning strategy\n",
    "        3. Regularization on latent space: currently tried sparse, $l_2$, and normal\n",
    "1. Experiments\n",
    "    1. Resnet-like structure generally improve test performance (but not much).\n",
    "    \n",
    "    [See some loss curves](./write-up/figs/loss_records)\n",
    "    \n",
    "    2. Running task-wise early stopping with smaller learning rate.\n",
    "    \n",
    "    [Ref: Facial Landmark Detection by Deep Multi-task Learning](https://link.springer.com/chapter/10.1007/978-3-319-10599-4_7)\n",
    "    \n",
    "    [Ref: PhD Thesis: Multitask learning](http://reports-archive.adm.cs.cmu.edu/anon/1997/CMU-CS-97-203.pdf)\n",
    "    ![Example of MTL on nine 1D-ALVINN tasks](./write-up/figs/eg_mtl_loss_curves.png)\n",
    "    \n",
    "    3. Running single task.\n",
    "    4. Using ID as main task, and others as auxillary tasks.\n",
    "    \n",
    "    Other tasks can be viewed as sub-tasks of ID.\n",
    "    \n",
    "    5. Redesign loss\n",
    "        \n",
    "        For regression: the likelihood $p(y_1 | f(x)) = \\mathcal{N}(f(x), \\sigma_1^2)$\n",
    "        \n",
    "        For classification: $p(y_2 | f(x)) = \\text{softmax}(\\frac{1}{\\sigma_2^2}f(x))$\n",
    "        \n",
    "        The total log-likelihood for both regression and classification tasks: $\\mathcal{L}(\\sigma_1, \\sigma_2) \\approx \\frac{1}{2\\sigma_1^2}\\|y_1 - f(x)\\|_2^2 - \\frac{1}{2\\sigma_2^2}\\log\\text{softmax}(y_2, f(x)) + \\log\\sigma_1^2 + \\log\\sigma_2^2$\n",
    "        \n",
    "        Hence the variance controlls the contribution of each task.\n",
    "        \n",
    "        Can further extend to generalized linear mixed model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update ~01/07/2018\n",
    "\n",
    "1. Experiments\n",
    "    1. Apply task-wise early stopping\n",
    "    \n",
    "        Loss reduced for ID, Dialect and Age.\n",
    "    2. Use wasserstein loss for Autoencoder\n",
    "        \n",
    "        Loss reduced for reconstruction. \n",
    "    3. Learning strategy: train AE more before training Discriminators\n",
    "    \n",
    "        Improved performance.\n",
    "    4. Conjecture: two adversaries in learning\n",
    "        1. Adversary between Reconstruction and Discrimination\n",
    "        \n",
    "            Consider X -f- z -g- X' and X -f- z -h- y: $(g \\circ f)(X) = X'$, $(h \\circ f)(X) = y$, then $f = g^{-1} \\circ I$ and $(h \\circ (g^{-1} \\circ I))(X) = y$. Hence the discrimination process invloves in inverting reconstruction process.\n",
    "            \n",
    "        2. Adversary among discriminators\n",
    "        \n",
    "            Each decoding process has true, underlying $\\{\\mathcal{Z}\\}$, but the encoded $\\{\\mathcal{\\widetilde{Z}}\\}$ differ with $\\{\\mathcal{Z}\\}$. There is a game among learning these latent representations.\n",
    "    \n",
    "    5. Todo: Apply probabilistic early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update ~01/08/2018\n",
    "\n",
    "1. Experiments\n",
    "    1. Dynamic task-wise stopping and resuming\n",
    "        \n",
    "        Use learning curve statistics to regularize learning: at step $t$, stop task $\\alpha$ if \n",
    "        \n",
    "        $$\\frac{\\text{med}_kE_{\\text{train}}^{\\alpha}}{\\text{mean}_kE_{\\text{train}}^{\\alpha} - \\text{med}_kE_{\\text{train}}^{\\alpha}} \\cdot \\frac{\\left|E_{\\text{test}}^{\\alpha}(t) - \\min_tE_{\\text{train}}\\right|}{\\lambda^{\\alpha}\\min_tE_{\\text{train}}} > \\text{threshold}$$,\n",
    "        \n",
    "        and resume if smaller than the threshold, where $\\text{med}_kE_{\\text{train}}^{\\alpha}$ is the median of the recent $k$ train errors for task $\\alpha$, $E_{\\text{test}}^{\\alpha}(t)$ is the test error at step $t$, $\\min_tE_{\\text{train}}$ is the minimum of the total $t$ train errors, and $\\lambda^{\\alpha}$ is the loss weight for task $\\alpha$.\n",
    "        \n",
    "    2. Loss\n",
    "        1. Use dual of wasserstein distance for reconstruction; mean absolute deviation for continuous attributes prediction --> weak convergence\n",
    "        2. Todo: use probabilistic formulation for all tasks and derive weaker metric;\n",
    "        \n",
    "            also use it to adjust dynamic task-wise stopping and resuming.\n",
    "            \n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech latent feature discovery for multi-label forensic profiling\n",
    "\n",
    "Project for IWBF 2018\n",
    "\n",
    "----\n",
    "\n",
    "**Objective:** Find latent representations in speech that can predict forensic aspects of human.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update ~12/11/2017\n",
    "\n",
    "1. Write-up:\n",
    "\n",
    "    https://www.overleaf.com/12702004wrrmdybmpqsq#/48448185/\n",
    "\n",
    "2. Implementation\n",
    "    1. Data: TIMIT, Alcohol, (SRE)\n",
    "    2. Model\n",
    "        ![model](./write-up/figs/model.png)  \n",
    "    3. Current attributes: speaker-id, gender, dialect, age, height, drunk\n",
    "    4. [Code](./src)\n",
    "    \n",
    "3. Final poster for convex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update ~12/18/2017\n",
    "\n",
    "1. Experiments\n",
    "    1. Currently using latent codes from autoencoder to do multi-label classification and regression.\n",
    "    2. It works well on certain tasks (reconstruction, gender, height), but not well on other tasks (id, dialect, age).\n",
    "    3. Including more datasets (SRE, Yandong's face data)\n",
    "\n",
    "large model openset (100 epochs, batch 4)\n",
    "\n",
    "|Label | Reconstruction Loss ($l_2^2$) | ID Acc (%) | Gender Acc (%) | Dialect Acc (%) | Age MSE | Height MSE |\n",
    "|:----|----:|---:|---:|---:|----:|---:|\n",
    "| Train | 0.1417 | 75.15 | 97.15 | 80.40 | 6.6931 | 0.0668 |\n",
    "| Test | 0.1847 | 3.59 | 92.83 | 14.73 | 81.1115 | 0.0818 |\n",
    " \n",
    "2. Writing paper\n",
    "\n",
    "3. Todo next week\n",
    "    1. Finish draft of the paper.\n",
    "    2. Improve the model.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update ~12/19/2017\n",
    "\n",
    "1. Experiments\n",
    "    1. Observations: ID, Age and Dialect seem to be correlated (similar trends of loss), whereas Gender and Height seem to be independent (not speaker-dependent).\n",
    "    2. Back to closed set of speakers -- openset is far more difficult.\n",
    "    3. Reduce model size to speed up training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update ~12/20/2017\n",
    "\n",
    "1. Experiments\n",
    "    1. Train on size-reduced model: significantly reduced model size, less layers, less parameters, smaller z dimension.\n",
    "    2. Use closed set speakers, split (0.8 / 0.2).\n",
    "    3. Observations: \n",
    "    \n",
    "small model openset (300 epochs, batch 4)\n",
    "\n",
    "|Label | Reconstruction Loss ($l_2^2$) | ID Err (%) | Gender Err (%) | Dialect Err (%) | Age MSE | Height MSE |\n",
    "|:----|----:|---:|---:|---:|----:|---:|\n",
    "| Train | 0.1168 | 0.98 | 0.02 | 1.45 | 1.1543 | 0.0980 |\n",
    "| Test | 0.1529 | 98.60 | 3.81 |  82.36 | 83.5361 | 0.0981 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update ~12/20/2017\n",
    "\n",
    "1. Experiments\n",
    "    1. Observations: while the train losses monotonically decreases, the test losses for id and dialect strangely increases.\n",
    "\n",
    "![strange-loss](./write-up/figs/strange_loss.png)\n",
    "\n",
    "small model closedset (300 epochs, batch 16)\n",
    "\n",
    "|Label | Reconstruction Loss ($l_2^2$) | ID Err (%) | Gender Err (%) | Dialect Err (%) | Age MSE | Height MSE |\n",
    "|:----|----:|---:|---:|---:|----:|---:|\n",
    "| Train | 0.1209 | 0.15 | 0.00 | 0.31 | 0.5237 | 0.0177 |\n",
    "| Test | 0.1609 | 87.54 | 0.0150 | 77.97 | 60.2400 | 0.0773 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update ~12/28/2017\n",
    "\n",
    "Did not do much this week ...\n",
    "\n",
    "1. Found the stupid bug in code, now all losses decreasing.\n",
    "    \n",
    "small model closedset (less speakers)\n",
    "\n",
    "|Label | Reconstruction Loss ($l_2^2$) | ID Err (%) | Gender Err (%) | Dialect Err (%) | Age MSE | Height MSE |\n",
    "|:----|----:|---:|---:|---:|----:|---:|\n",
    "| Train |  | 32.40 |  |  |  |  |\n",
    "| Test |  | 48.33 | | |  ||\n",
    "\n",
    "2. Paper\n",
    "\n",
    "    https://www.overleaf.com/12702004wrrmdybmpqsq#/48448185/\n",
    "\n",
    "3. Todo\n",
    "\n",
    "    1. Extend to multi-datasets\n",
    "    2. Have to finish the paper !!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Update ~12/30/2017\n",
    "\n",
    "1. Multitask learning (MTL)\n",
    "    1. Models\n",
    "        1. Neural models\n",
    "            1. Hard parameter sharing\n",
    "            2. Soft parameter sharing\n",
    "            3. **what to share in model?**\n",
    "\n",
    "                ** sharing information with unrelated tasks might impede learning individual tasks**\n",
    "        2. Non-neural models [assume model parameter $W$ of size ($d$-feature dims $\\times$ $T$-tasks)]\n",
    "            1. sparsity across tasks through norm regularization\n",
    "                1. block-sparse regularization: mixed $l_1 / l_q$ norms\n",
    "                2. group lasso\n",
    "                3. trace norm regularize: low rank\n",
    "                4. combine block-sparse and element-wise sparse\n",
    "            2. modelling the relationships between tasks\n",
    "                1. clustering constraints: task mean and between-task variance; possibly add within-task variance\n",
    "                2. more complex structures: graph, tree, k-nn\n",
    "                3. Bayesian\n",
    "                    1. Gaussian process with shared covariance\n",
    "                    2. mean task-dependent and a clustering of the tasks using a mixture distribution\n",
    "                    3. Dirichlet process and enable the model to learn the similarity between tasks as well as the number of clusters\n",
    "                    4. hierarchical Bayesian model -- a latent task hierarchy\n",
    "                    5. actual tasks are linear combination of small number of latent basis tasks\n",
    "        3. Auxiliary tasks\n",
    "            1. related task\n",
    "            2. adversarial task\n",
    "            3. hints: predict features\n",
    "            4. attention\n",
    "            5. quantization smoothing\n",
    "            6. representation learning\n",
    "            7. **what auxiliary tasks are helpful?**\n",
    "            \n",
    "            e.g.  two tasks are $\\mathcal{F}$-related if the data for both tasks can be generated from a fixed probability distribution using a set of transformations $\\mathcal{F}$\n",
    "            \n",
    "            Task similarity is not binary, but resides on a spectrum. More similar tasks should help more in MTL, while less similar tasks should help less. Allowing our models to learn what to share with each task might allow us to temporarily circumvent the lack of theory and make better use even of only loosely related tasks. However, we also need to develop a more principled notion of task similarity with regard to multi-task learning in order to know which tasks we should prefer.\n",
    "            \n",
    "             **Existing problems**: task similarity, relationship, hierarchy, and benefit for MTL\n",
    "\n",
    "2. **Multitask learning and Adversarial learning**\n",
    "    1. Domain adaptation ?\n",
    "    2. MTL with loosely related tasks: multiple adversarial learning?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update ~01/01/2018\n",
    "\n",
    "1. Address the problems analyzed in **Update ~12/30/2017**\n",
    "    1. Need a sharing mechanism among tasks\n",
    "    2. Need a latent domain adaptation mechanism\n",
    "    3. Weighted loss w.r.t. the task uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update ~01/02/2018\n",
    "\n",
    "1. Experiments\n",
    "    0. **Two difficulties**\n",
    "        1. Model was not able to handle single but difficult task (e.g., id, age), neither multiple tasks. --> need better model\n",
    "        2. The tasks are loosely relevant. Some tasks bias other tasks. --> need proper model sharing, loss weighting, auxillary tasks\n",
    "    1. Tried different sharing mechanisms but not work\n",
    "        1. sharing part of encoder but with different latent codes for different tasks\n",
    "        2. sharing latent codes (including varying latent code distributions, e.g. Gaussians with centered mean but different variances)\n",
    "        3. sharing part of decoder\n",
    "    2. Changed model to Resnet-like structure\n",
    "        1. combine multi-resolution features\n",
    "    3. Applying weighting to different task losses\n",
    "        1. hard-assigned weights: assign larger weights to tough tasks.\n",
    "        2. soft-assigned weights: probabilistically reformulate the task objectives (for regression, model likelihood as a Gaussian; for classification, use softmax), and then use objective variance / uncertainty to adjust weights.\n",
    "    4. Trying multi-stage autoencoders\n",
    "    5. Attention model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update ~01/03/2018\n",
    "\n",
    "1. Experiments\n",
    "    1. Resnet-like structure seems to improve accuracy than standard CNN.\n",
    "    2. Use task-wise learning rates and task-wise early stopping.\n",
    "    3. Use regularization on shared latent representation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update ~01/04/2018\n",
    "\n",
    "0. Conclusions\n",
    "    1. Based on previous observations and discussion, we need\n",
    "        0. Model: Resnet, probably add attention\n",
    "        1. Model sharing: must share from latent level\n",
    "        2. Related tasks: design auxillary tasks, corresponding loss and learning strategy\n",
    "        3. Regularization on latent space: currently tried sparse, $l_2$, and normal\n",
    "1. Experiments\n",
    "    1. Resnet-like structure generally improve test performance (but not much).\n",
    "    \n",
    "    [See some loss curves](./write-up/figs/loss_records)\n",
    "    \n",
    "    2. Running task-wise early stopping with smaller learning rate.\n",
    "    \n",
    "    [Ref: Facial Landmark Detection by Deep Multi-task Learning](https://link.springer.com/chapter/10.1007/978-3-319-10599-4_7)\n",
    "    \n",
    "    [Ref: PhD Thesis: Multitask learning](http://reports-archive.adm.cs.cmu.edu/anon/1997/CMU-CS-97-203.pdf)\n",
    "    ![Example of MTL on nine 1D-ALVINN tasks](./write-up/figs/eg_mtl_loss_curves.png)\n",
    "    \n",
    "    3. Running single task.\n",
    "    4. Using ID as main task, and others as auxillary tasks.\n",
    "    \n",
    "    Other tasks can be viewed as sub-tasks of ID.\n",
    "    \n",
    "    5. Redesign loss\n",
    "        \n",
    "        For regression: the likelihood $p(y_1 | f(x)) = \\mathcal{N}(f(x), \\sigma_1^2)$\n",
    "        \n",
    "        For classification: $p(y_2 | f(x)) = \\text{softmax}(\\frac{1}{\\sigma_2^2}f(x))$\n",
    "        \n",
    "        The total log-likelihood for both regression and classification tasks: $\\mathcal{L}(\\sigma_1, \\sigma_2) \\approx \\frac{1}{2\\sigma_1^2}\\|y_1 - f(x)\\|_2^2 - \\frac{1}{2\\sigma_2^2}\\log\\text{softmax}(y_2, f(x)) + \\log\\sigma_1^2 + \\log\\sigma_2^2$\n",
    "        \n",
    "        Hence the variance controlls the contribution of each task.\n",
    "        \n",
    "        Can further extend to generalized linear mixed model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update ~01/07/2018\n",
    "\n",
    "1. Experiments\n",
    "    1. Apply task-wise early stopping\n",
    "    \n",
    "        Loss reduced for ID, Dialect and Age.\n",
    "    2. Use wasserstein loss for Autoencoder\n",
    "        \n",
    "        Loss reduced for reconstruction. \n",
    "    3. Learning strategy: train AE more before training Discriminators\n",
    "    \n",
    "        Improved performance.\n",
    "    4. Conjecture: two adversaries in learning\n",
    "        1. Adversary between Reconstruction and Discrimination\n",
    "        \n",
    "            Consider X -f- z -g- X' and X -f- z -h- y: $(g \\circ f)(X) = X'$, $(h \\circ f)(X) = y$, then $f = g^{-1} \\circ I$ and $(h \\circ (g^{-1} \\circ I))(X) = y$. Hence the discrimination process invloves in inverting reconstruction process.\n",
    "            \n",
    "        2. Adversary among discriminators\n",
    "        \n",
    "            Each decoding process has true, underlying $\\{\\mathcal{Z}\\}$, but the encoded $\\{\\mathcal{\\widetilde{Z}}\\}$ differ with $\\{\\mathcal{Z}\\}$. There is a game among learning these latent representations.\n",
    "    \n",
    "    5. Todo: Apply probabilistic early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update ~01/08/2018\n",
    "\n",
    "1. Experiments\n",
    "    1. Dynamic task-wise stopping and resuming\n",
    "        \n",
    "        Use learning curve statistics to regularize learning: at step $t$, stop task $\\alpha$ if \n",
    "        \n",
    "        $$\\frac{\\text{med}_kE_{\\text{train}}^{\\alpha}}{\\left|\\text{mean}_kE_{\\text{train}}^{\\alpha} - \\text{med}_kE_{\\text{train}}^{\\alpha}\\right|} \\cdot \\frac{\\left|E_{\\text{test}}^{\\alpha}(t) - \\min_tE_{\\text{train}}\\right|}{\\lambda^{\\alpha}\\min_tE_{\\text{train}}} > \\text{threshold}$$,\n",
    "        \n",
    "        and resume if smaller than the threshold, where $\\text{med}_kE_{\\text{train}}^{\\alpha}$ is the median of the recent $k$ train errors for task $\\alpha$, $E_{\\text{test}}^{\\alpha}(t)$ is the test error at step $t$, $\\min_tE_{\\text{train}}$ is the minimum of the total $t$ train errors, and $\\lambda^{\\alpha}$ is the loss weight for task $\\alpha$.\n",
    "        \n",
    "    2. Loss\n",
    "        1. Use dual of wasserstein distance for reconstruction; mean absolute deviation for continuous attributes prediction --> weak convergence\n",
    "        2. Todo: use probabilistic formulation for all tasks and derive weaker metric;\n",
    "        \n",
    "            also use it to adjust dynamic task-wise stopping and resuming.\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update ~01/10/2018\n",
    "\n",
    "0. Conclusions\n",
    "    1. Different task has different learning pace, and aids main task in different phase.\n",
    "    2. Wake and Sleep phase existed in learning.\n",
    "        \n",
    "        The following figures show during the learning, there exists **wake phase (level 0) and sleep phase (level 1)** for different tasks.\n",
    "        \n",
    "        All figures are from Resnet-like structure on TIMIT data, using Wasserstein distance for reconstruction, mean absolute deviation for age and height, with **different dynamic stopping and resuming thresholds**.\n",
    "        \n",
    "        The **Purpule and Red** curves represent currently best performance.\n",
    "        \n",
    "        Wake and sleep for age and dialect. \n",
    "        ![age_dia_stop](./write-up/figs/loss_records/age_dia_stop.png)\n",
    "        \n",
    "        Wake and sleep for gender and height. \n",
    "        ![gen_hgt_stop](./write-up/figs/loss_records/gen_hgt_stop.png)\n",
    "        \n",
    "        Wake and sleep for id. \n",
    "        ![id_stop](./write-up/figs/loss_records/id_stop.png)\n",
    "    \n",
    "1. Experiments\n",
    "    1. Dynamic stopping and resuming significantly reduces multi-task test errors.\n",
    "    \n",
    "300 epochs\n",
    "\n",
    "|Label | Reconstruction Loss ($Wass_1$) | ID Err (%) | Gender Err (%) | Dialect Err (%) | Age MAE (yr) | Height MAE (cm) |\n",
    "|:----|----:|---:|---:|---:|----:|---:|\n",
    "| Train | 0.2210 | 28.87 | 1.76 | 47.27 | 5.298 | 0.2647 |\n",
    "| Test | 0.2508 | 39.76 | 1.03 | 68.60 | 5.405 | 0.2724 |\n",
    "\n",
    "[See some loss curves (train_err_id_age_dynamic_stop ...) (Red and Purple curves are latest.)](./write-up/figs/loss_records)\n",
    "    \n",
    "    2. Currently using task variance to regularize wake and sleep, task weights, and task learning rates.\n",
    "    \n",
    "    Results pending.\n",
    "    \n",
    "    3. More auxiliary tasks may help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update ~01/16/2018\n",
    "\n",
    "1. Experiments\n",
    "    1. Task variance prediction & use variance to adjust dynamic stopping as well as learning rates.\n",
    "        \n",
    "        $\\Sigma_t = (h_{\\theta_t}^*)^2 - (h_{\\theta_t}^*h_{\\theta_t})^2[h_{\\theta_t}^2 + \\sigma_t^2]^{-1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update ~01/19/2018\n",
    "\n",
    "1. Experiments\n",
    "    1. Use task variance to control task loss weights and dynamic stopping -> reduced age and height loss, seems work for regression.\n",
    "    2. Test hypothesis: for a rival task pair (A, B), the loss for task A stops descreasing because task B hinges task A; stopping task B helps the loss of task A to resume decreasing. For a friends pair (A, B), the loss for task A decreases further if task B joins A, and vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update ~01/22/2018\n",
    "\n",
    "1. Experiments\n",
    "    1. Continue testing hypothesis on friend and rival pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update ~01/24/2018\n",
    "\n",
    "1. Experiments\n",
    "    1. Finished a thorough series of tests:\n",
    "        1. H1: Friend task pair (A, B) helps each other to reduce loss, namely, if A has a high variance, joining B reduces loss(A), and vice versa.\n",
    "        \n",
    "        Friend pair shares similar optimization paths, joining a friend task adjusts update steps and provides larger gradients. \n",
    "        \n",
    "        Test 1: (Hgt, Gen), (Hgt, Age), (Hgt, Dia)\n",
    "        \n",
    "        2. H2: Rival task pair (A, B) pulls each other off from optimal path to high error path; loss for stronger one reduces, while loss for weaker one increases.\n",
    "        \n",
    "        Rival pair has diverse optimization path, joining a rival leads to competing update directions.\n",
    "        \n",
    "        Test 2: (Id, Hgt), (Id, Dia), (Id, Age)\n",
    "        \n",
    "        3. H2-2: For a rival pair (A, B), stopping one helps the loss of the other to drop, and vice versa.\n",
    "        \n",
    "        Test 2-2: (Id, Hgt), (Id, Dia), (Id, Age)\n",
    "        \n",
    "    2. Performance improved.\n",
    "    \n",
    "2. Theory\n",
    "    1. Derive a manifold view of the multi-task learning scheme (can also adopted to adversarial learning scheme).\n",
    "    \n",
    "    ![manifold-view](./write-up/figs/manifold_view.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update ~01/28/2018\n",
    "\n",
    "1. Conclusions from experiments\n",
    "    1. Although we do not yet have a way describing / quantitizing the relation and hierarchy among the tasks, the pair-wise experiments show that the tasks have a *relative closeness*: \n",
    "\n",
    "    ![task-relation](./write-up/figs/task_relation.png)\n",
    "    \n",
    "    We do not want to model the *relative closeness* since it's dynamic, complicated and task-dependent. Rather, we seek for an alternative that is task-independent.\n",
    "    \n",
    "    2. Dynamic stopping does not help much in these pair-wise experiments.\n",
    "    \n",
    "2. Theories\n",
    "    \n",
    "    We develop a link between the *curves on the latent manifold* and the *loss maps*; we want to characterize the relation between the *direction of these curves* (*tangent bundles*) and the *loss trends*.\n",
    "    1. Describe the joint loss manifold\n",
    "    2. Describe the maps among manifolds\n",
    "    3. Describe the maps between the curves on latent manifold and the loss manifold\n",
    "    4. Characterize the tangent bundle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update ~01/30/2018\n",
    "\n",
    "1. Hypos, experiments and conclusions\n",
    "    1. H1: For a fixed latent manifold $\\mathcal{Z}$, each task in the multi-dicrimination process is independent of the others, follows its own optimization path, and reaches to its own optimal.\n",
    "    2. T1: Train an optimal AE first, fix Z, then train on multi-tasks. Test on single task, two-task pairs, and all tasks.\n",
    "    3. C1: H1 negative. Even for fixed Z, the tasks intertwine with each other. Ex., for err_id, id+age < all; for err_age, all < age+id. In addition, compared to simultaneously train AE and Ds, fixing AE first and then training Ds generally leads to slower learning of dia, id, hgt, but faster on age, gen. This shows that dia, id, and hgt are indeed hard to learn, while gen and age are relatively easy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update ~02/04/2018\n",
    "\n",
    "1. Modeling dynamics in multitask learning\n",
    "\n",
    "    ![manifold_model](./write-up/figs/manifold_model.png)\n",
    "\n",
    "\n",
    "- The same theory can be applied to adversarial learning using **Morse flow**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
